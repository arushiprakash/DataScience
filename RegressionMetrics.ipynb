{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics\n",
    "Sklearn provides a number of metris for scoring regression algorithms. Let us take a look at each one of them. Inspired from [this article](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)\n",
    "\n",
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%maptlotlib` not found.\n"
     ]
    }
   ],
   "source": [
    "%maptlotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data set\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', \n",
    "         'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', \n",
    "         'LSTAT', 'MEDV']\n",
    "data = pd.read_csv('data/BostonHousing.csv',delim_whitespace=True, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "Calculates the absolute value of deviations from the actual dataset. Does not indicate under/over-predicting data, but only the magnitude of error. In the sklearn implementation, we get the negative, same as log loss. \n",
    "\n",
    "**Good** - It is rbust to outliers, since unlike the MSE, it does not give too much weight to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -4.005 (2.084)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    }
   ],
   "source": [
    "array = data.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"MAE: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "**Good** - This has nicer mathematical properties such that graients can be calculates and with gradient descent we can locations of lowest error.\n",
    "**Limitations** - Since squares gives much higher weight to the points with higher deviations from the actual value. But this can be turned around to be a features when you are concerned about outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -34.705 (45.574)\n"
     ]
    }
   ],
   "source": [
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"MSE: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R^2 Metric\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=1&space;-&space;\\frac{RSS}{TSS}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?1&space;-&space;\\frac{RSS}{TSS}\" title=\"1 - \\frac{RSS}{TSS}\" /></a> \n",
    "Indication of the goodness of fit of the predictions from the actual value. As you increas the number of predictors in your model, the R^2 always increases, and the adjusted R^2 value should be used.  \n",
    "This value can be evn be negative if the model is arbitrarily worse.Closer to 1.0 values are preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.203 (0.595)\n"
     ]
    }
   ],
   "source": [
    "scoring = 'r2'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"R^2: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
