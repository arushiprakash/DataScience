{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Ensemble Learning\n",
    "Ensemble learning techniques are used to improve classification/regressions in several ways. For a comprehensive guide, please see [https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/](here).   \n",
    "\n",
    "In this notebook, I am going to practice some of them on test datasets.\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and manipulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "data = pd.read_csv('data/iris.csv', names=names)\n",
    "X = data[['sepal-length', 'sepal-width', 'petal-length', 'petal-width']]\n",
    "Y = data[['class']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.3, random_state=420)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing single models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.60      0.75        20\n",
      " Iris-virginica       0.56      1.00      0.71        10\n",
      "\n",
      "      micro avg       0.82      0.82      0.82        45\n",
      "      macro avg       0.85      0.87      0.82        45\n",
      "   weighted avg       0.90      0.82      0.83        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.90      0.95        20\n",
      " Iris-virginica       0.83      1.00      0.91        10\n",
      "\n",
      "      micro avg       0.96      0.96      0.96        45\n",
      "      macro avg       0.94      0.97      0.95        45\n",
      "   weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors\n",
    "model = KNeighborsClassifier(2)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.90      0.95        20\n",
      " Iris-virginica       0.83      1.00      0.91        10\n",
      "\n",
      "      micro avg       0.96      0.96      0.96        45\n",
      "      macro avg       0.94      0.97      0.95        45\n",
      "   weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.90      0.95        20\n",
      " Iris-virginica       0.83      1.00      0.91        10\n",
      "\n",
      "      micro avg       0.96      0.96      0.96        45\n",
      "      macro avg       0.94      0.97      0.95        45\n",
      "   weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive-Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Ensembling Techniques\n",
    "Here we implement 3 simple ensembling techniques implemented through VotingClassifier:\n",
    "1. **Voting** - Every model assigns a probability of being in a certain class. Based on these probabilities(0.45 0.45 0.90 = 2 against, 1 for), we count the majority decision.\n",
    "2. **Averaging** - In the previous case, we would take the average probability (=0.60), and decide that the point lies in that class\n",
    "3. **Weighted Avergae** - Instead of taking a regular average, we can give one model more preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.90      0.95        20\n",
      " Iris-virginica       0.83      1.00      0.91        10\n",
      "\n",
      "      micro avg       0.96      0.96      0.96        45\n",
      "      macro avg       0.94      0.97      0.95        45\n",
      "   weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Voting\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = KNeighborsClassifier(2)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('knn', model2)], voting='hard')\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.85      0.92        20\n",
      " Iris-virginica       0.77      1.00      0.87        10\n",
      "\n",
      "      micro avg       0.93      0.93      0.93        45\n",
      "      macro avg       0.92      0.95      0.93        45\n",
      "   weighted avg       0.95      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Averaging\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = KNeighborsClassifier(2)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('knn', model2)], voting='soft')\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.80      0.89        20\n",
      " Iris-virginica       0.71      1.00      0.83        10\n",
      "\n",
      "      micro avg       0.91      0.91      0.91        45\n",
      "      macro avg       0.90      0.93      0.91        45\n",
      "   weighted avg       0.94      0.91      0.91        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weighted - Averaging\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = KNeighborsClassifier(2)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('knn', model2)], voting='soft', weights=[2,1])\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        15\n",
      "Iris-versicolor       1.00      0.85      0.92        20\n",
      " Iris-virginica       0.77      1.00      0.87        10\n",
      "\n",
      "      micro avg       0.93      0.93      0.93        45\n",
      "      macro avg       0.92      0.95      0.93        45\n",
      "   weighted avg       0.95      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weighted - Averaging\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = KNeighborsClassifier(2)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('knn', model2)], voting='soft', weights=[1,2])\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** - We used simple ensemble techniques to improve classification. Simple voting provided the best result. However, we can get similar results, when we give more weight to the kNN classifier using weighted averages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
