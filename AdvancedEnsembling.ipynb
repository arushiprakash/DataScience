{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ensemble Learning\n",
    "Ensemble learning techniques are used to improve classification/regressions in several ways. For a comprehensive guide, please see [https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/](here).   \n",
    "\n",
    "In this notebook, I am going to practice some of the advanced techniques like \n",
    "1. Stacking\n",
    "2. Blending\n",
    "3. Bagging\n",
    "4. Boosting\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "    \n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and manipulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "data = pd.read_csv('data/iris.csv', names=names)\n",
    "X = data[['sepal-length', 'sepal-width', 'petal-length', 'petal-width']]\n",
    "Y = data[['class']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.3, random_state=20)\n",
    "\n",
    "# Normalizing the data\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(x_train)\n",
    "#x_train = scaler.transform(x_train)\n",
    "#x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Ensembling Techniques\n",
    "Here we implement some advanced ensembling techniques:\n",
    "1. **Stacking** - [more here](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/). Using base models, does predictions on sections of training data (stratified), and finally the whole data. The base models are combined by using Logistic regression on the predictions of all base models.\n",
    "2. **Blending** - The data is split into train, validation, and testing. Base models are fit onto the trainig set, and predictions are made on the validation and test sets. Final makes fits on the validation set, and predicts on the training set.\n",
    "Bagging-type\n",
    "3. **Bagging** or Bootstrap Aggregating - Different samples are taken with replacement, and several models are applied on it, and a base model combines all the answers together.\n",
    "4. **Random Forest** - Like bagging, it bootstraps data points, but also selects some features for every model and finally combines all the predictions with a decision tree.\n",
    "Boosting-type\n",
    "5. **AdaBoost** - Models are built on a subset of data and erros are calculated. In the next model, data points that are incorrectly predicted are given more weight. This process is repeated until the error function becomes constant.\n",
    "6. \n",
    "\n",
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stacking(model,train,y,test,n_fold):\n",
    "    \"\"\"Performs stacking on given data\"\"\"\n",
    "    folds = StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "    test_pred = []\n",
    "    train_pred = []\n",
    "    for train_indices, val_indices in folds.split(train,y.values):\n",
    "        x_train, x_val = train.iloc[train_indices], train.iloc[val_indices]\n",
    "        y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n",
    "\n",
    "        model.fit(X=x_train,y=y_train)\n",
    "        train_pred = np.append(train_pred,model.predict(x_val))\n",
    "    model.fit(train, y.values); model.predict(test);\n",
    "    test_pred = np.append(test_pred,model.predict(test))\n",
    "    return test_pred, train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "test_pred1, train_pred1 =Stacking(model=model1,n_fold=5, train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred1 = pd.DataFrame(train_pred1)\n",
    "test_pred1 = pd.DataFrame(test_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "model2 = KNeighborsClassifier(2)\n",
    "\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=5,train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred2 = pd.DataFrame(train_pred2)\n",
    "test_pred2 = pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of stacked model =  0.88889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "# Since this a categorical variable have to one hot encode\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(df_test)\n",
    "enc_df_test = enc.transform(df_test)\n",
    "enc_df = enc.transform(df)\n",
    "\n",
    "\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(enc_df,y_train)\n",
    "print \"Score of stacked model = %8.5f\" %(model.score(enc_df_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:16: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Test-train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "# Train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "# Fittin first model\n",
    "model1 = DecisionTreeClassifier(random_state=1)\n",
    "model1.fit(x_train, y_train)\n",
    "val_pred1 = model1.predict(x_val)\n",
    "test_pred1 = model1.predict(x_test)\n",
    "val_pred1 = pd.DataFrame(val_pred1)\n",
    "test_pred1 = pd.DataFrame(test_pred1)\n",
    "\n",
    "# Fitting second model\n",
    "model2 = KNeighborsClassifier(2)\n",
    "model2.fit(x_train,y_train)\n",
    "val_pred2 = model2.predict(x_val)\n",
    "test_pred2 = model2.predict(x_test)\n",
    "val_pred2 = pd.DataFrame(val_pred2)\n",
    "test_pred2 = pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of blended model =  0.96667\n"
     ]
    }
   ],
   "source": [
    "# Combining models\n",
    "df = pd.concat([val_pred1, val_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "# Since this a categorical variable have to one hot encode\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(df_test)\n",
    "enc_df_test = enc.transform(df_test)\n",
    "enc_df = enc.transform(df)\n",
    "\n",
    "\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(enc_df,y_val)\n",
    "print \"Score of blended model = %8.5f\" %(model.score(enc_df_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of bagging model with decision trees =  0.96667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/ensemble/bagging.py:618: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "# Choosing with replacement, and fitting decision tree on each dataset\n",
    "# Final answer by voting\n",
    "model = BaggingClassifier(DecisionTreeClassifier(random_state=1))\n",
    "model.fit(x_train, y_train)\n",
    "print \"Score of bagging model with decision trees = %8.5f\" %(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of bagging model with kNN =  1.00000\n"
     ]
    }
   ],
   "source": [
    "model = BaggingClassifier(KNeighborsClassifier(2))\n",
    "model.fit(x_train, y_train)\n",
    "print \"Score of bagging model with kNN = %8.5f\" %(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of random forest =  0.96667\n"
     ]
    }
   ],
   "source": [
    "# Choosing with replacement, and fitting decision tree on each dataset\n",
    "# Final answer by voting\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "print \"Score of random forest = %8.5f\" %(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of AdaBoost =  0.96667\n"
     ]
    }
   ],
   "source": [
    "# Decision trees as base estmators\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "print \"Score of AdaBoost = %8.5f\" %(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Gradient Boosting =  0.96667\n"
     ]
    }
   ],
   "source": [
    "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "print \"Score of Gradient Boosting = %8.5f\" %(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "Boosting with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of XGBoosting =  0.96667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jimpfaendtner/Library/Python/2.7/lib/python/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(x_train, y_train)\n",
    "print \"Score of XGBoosting = %8.5f\" %(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** - For the toy Iris dataset, we used different, advanced ensemble techniques. Most prediction scores were 0.96 or greater, so all models performed really well. Notably, stacked models performed poorly (0.88) and Bagging classifer with NearestNeighborsClassifier worked the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
